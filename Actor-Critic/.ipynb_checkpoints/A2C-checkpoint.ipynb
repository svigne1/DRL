{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A2C, Advantage Actor Critic\n",
    "\n",
    "We are using a Advantage Actor Critic which <br/>\n",
    "- Uses **Bootstrapped** estimates for it's critcs' targets. And NOT Monte-Carlo estimates.\n",
    "- Takes a batch of gradients for training, NO parallelism in sampling or Asynchronuous sampling.\n",
    "- Has a **seperate network** for Actor and Critic. No weight sharing between the networks.\n",
    "- Simple **1-step return** as the Advantage (Bias). Not n-step (n = length of episode) where the critic is used for estimating only the baseline (Variance) or average of many n-step returns - GAE, where we take a bias-variance trade-off\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state 4 action 2\n"
     ]
    }
   ],
   "source": [
    "GAMMA = 0.99\n",
    "\n",
    "env = gym.make(\"CartPole-v0\")\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n\n",
    "\n",
    "print(\"state\",state_size, \"action\", action_size)\n",
    "\n",
    "STORE_PATH = '/Users/SV/Desktop/Lyra/CS285/Actor-Critic'\n",
    "logger = tf.summary.create_file_writer(STORE_PATH + f\"/AC-CartPole_{dt.datetime.now().strftime('%d%m%Y%H%M')}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor = keras.Sequential([\n",
    "    keras.layers.Dense(30, activation='relu', kernel_initializer=keras.initializers.he_normal()),\n",
    "    keras.layers.Dense(30, activation='relu', kernel_initializer=keras.initializers.he_normal()),\n",
    "    keras.layers.Dense(action_size, activation='softmax')\n",
    "])\n",
    "\n",
    "critic = keras.Sequential([\n",
    "    keras.layers.Dense(30, activation='relu', kernel_initializer=keras.initializers.he_normal()),\n",
    "    keras.layers.Dense(30, activation='relu', kernel_initializer=keras.initializers.he_normal()),\n",
    "    keras.layers.Dense(1)\n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Sample Action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since only one state is there, N = 1 in the mini-batch.\n",
    "# And index 0, gives the value for that state.     \n",
    "\n",
    "def sample_action(state):\n",
    "    softmax_out = actor(state.reshape((1, -1)))\n",
    "    selected_action = np.random.choice(action_size, p=softmax_out.numpy()[0])\n",
    "    return selected_action\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Sample Episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rewards2go(raw_rewards):\n",
    "    reward_sum = 0\n",
    "    result = []\n",
    "    for reward in reversed(raw_rewards):\n",
    "        reward_sum = reward + GAMMA * reward_sum\n",
    "        result.append(reward_sum)\n",
    "    return result.reverse()\n",
    "\n",
    "def sample_episode(states, next_states, rewards, rewards2go, actions):\n",
    "    state = env.reset()\n",
    "    raw_rewards = []\n",
    "    while True:\n",
    "        action = sample_action(state)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        \n",
    "        states.append(state)\n",
    "        next_states.append(next_state)\n",
    "        actions.append(action)\n",
    "        raw_rewards.append(reward)\n",
    "        next_state = state        \n",
    "        \n",
    "        if done:\n",
    "            rewards.extend(raw_rewards)\n",
    "            rewards2go.extend(get_rewards2go(raw_rewards))\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-15896bbea1c4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0mN\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;32mwhile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0msample_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m         \u001b[0msample_episode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards2go\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0mN\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;36m10\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-9eec46cae5ee>\u001b[0m in \u001b[0;36msample_episode\u001b[0;34m(states, next_states, rewards, rewards2go, actions)\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0mrewards\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_rewards\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m             \u001b[0mrewards2go\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_rewards2go\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_rewards\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not iterable"
     ]
    }
   ],
   "source": [
    "sample_size = 15000\n",
    "batch_size = 5120\n",
    "steps = 1000\n",
    "\n",
    "actor_optimizer = tf.keras.optimizers.Adam(lr=0.001)\n",
    "\n",
    "def averageGradients(grads, N):\n",
    "    for i in range(len(grads)):\n",
    "        grads[i] = grads[i]/N\n",
    "        \n",
    "        \n",
    "def addGradients(grads, batch_grads):\n",
    "    sum_grad = []\n",
    "    for (grad, batch_grad) in zip(grads, batch_grads):\n",
    "        sum_grad.append(grad + batch_grad)\n",
    "    return sum_grad\n",
    "\n",
    "def generate_sample_batcher(states, next_states, rewards, rewards2go, actions):\n",
    "    rewards = np.array(rewards)\n",
    "    rewards2go = np.array(rewards2go)\n",
    "    states = np.array(states)\n",
    "    next_states = np.array(next_states)\n",
    "    actions = np.array(actions)\n",
    "    \n",
    "    # Policy Gradients     \n",
    "    baseline = np.mean(rewards2go)\n",
    "    rewards2go = rewards2go - baseline\n",
    "    \n",
    "    def sample_batcher(n):\n",
    "        return states[n:n+batch_size], next_states[n:n+batch_size], rewards[n:n+batch_size], rewards2go[n:n+batch_size], actions[n:n+batch_size]\n",
    "\n",
    "    return sample_batcher\n",
    "    \n",
    "for step in range(steps):\n",
    "\n",
    "    # This is one gradient step    \n",
    "\n",
    "    # Numpy arrays are immutable. \n",
    "    # So these NEED to be python lists.\n",
    "    # Once sampled, u can convert them into np arrays\n",
    "    rewards = []\n",
    "    rewards2go = []\n",
    "    states = []\n",
    "    actions = []\n",
    "    next_states = []\n",
    "\n",
    "    N = 0 \n",
    "    while(len(states) < sample_size):\n",
    "        sample_episode(states, next_states, rewards, rewards2go, actions)\n",
    "        N += 1\n",
    "        if(N%10 == 0):\n",
    "            print(\"Sampling Episode - \", N)\n",
    "    print(\"Sampled\", N, \"Episodes\")\n",
    "\n",
    "    avg_reward = np.sum(rewards) / N    \n",
    "    bat_per_epoch = math.floor(len(states) / batch_size)\n",
    "    sample_batcher = generate_sample_batcher(rewards, rewards2go, states, next_states, actions)\n",
    "    \n",
    "    ###### Train Critic ######\n",
    "    \n",
    "#     # NEW optimizer for each policy to forget old policy's gradients.\n",
    "#     critic_optimizer = tf.keras.optimizers.Adam(lr=0.001)\n",
    "    \n",
    "#     # @t1, fit critic, rest of the time, take a single gradient-step     \n",
    "#     epoch = 300\n",
    "#     if step != 0:\n",
    "#         epoch = 4\n",
    "        \n",
    "#     for j in range(epoch):\n",
    "        \n",
    "#         gradients = None\n",
    "#         loss = 0 \n",
    "#         for i in range(bat_per_epoch): \n",
    "            \n",
    "#             # Sample a batch sequentially not randomnly             \n",
    "#             n = i*batch_size\n",
    "#             batch_states, batch_next_states, batch_rewards, batch_rewards2go, batch_actions = sample_batcher(n)            \n",
    "            \n",
    "#             # Target for critic             \n",
    "#             # @t1, Monte-Carlo estimates, rest of the time, Bootstrapped rewards.\n",
    "#             y_true = batch_rewards2go\n",
    "#             if step != 0:\n",
    "#                 y_true = batch_rewards + GAMMA * np.squeeze(critic(batch_next_states))\n",
    "            \n",
    "#             with tf.GradientTape() as tape:\n",
    "\n",
    "#                 # Squeeze is to remove dimension of size 1                 \n",
    "#                 # if we use np.squeeze, automatic differentiation wont work\n",
    "#                 y_pred = tf.squeeze(critic(batch_states)) \n",
    "                    \n",
    "#                 batch_loss = tf.keras.losses.MSE(y_true=y_true, y_pred=y_pred)\n",
    "                \n",
    "#             batch_gradients = tape.gradient(batch_loss, critic.trainable_variables)\n",
    "#             loss += batch_loss\n",
    "            \n",
    "#             # Sum the Gradients over all batches\n",
    "#             if gradients is None:\n",
    "#                 gradients = batch_gradients\n",
    "#             else:\n",
    "#                 gradients = addGradients(gradients, batch_gradients)\n",
    "                \n",
    "#         critic_optimizer.apply_gradients(zip(gradients, critic.trainable_variables))  \n",
    "#         if j%10 == 0:\n",
    "#             print(f\"Training critic for {j+1} epochs \", loss)\n",
    "\n",
    "    ###### Train Actor ######\n",
    "    \n",
    "    # For Policy Gradients\n",
    "    \n",
    "    gradients = None\n",
    "    for i in range(bat_per_epoch):\n",
    "        n = i*batch_size\n",
    "        batch_states, batch_next_states, batch_rewards, batch_rewards2go, batch_actions = sample_batcher(n)\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions = actor(batch_states)\n",
    "            batch_loss = tf.keras.losses.sparse_categorical_crossentropy(y_true=batch_actions, y_pred=predictions, from_logits=False)\n",
    "\n",
    "            # For Actor-Critic\n",
    "#             advantage = batch_rewards + GAMMA * np.squeeze(critic(batch_next_states)) - np.squeeze(critic(batch_states))\n",
    "#             batch_loss = batch_loss * advantage\n",
    "\n",
    "            # For Polciy Gradients\n",
    "            batch_loss = batch_loss * batch_rewards2go\n",
    "            \n",
    "        batch_gradients = tape.gradient(batch_loss, actor.trainable_variables)\n",
    "\n",
    "        # Sum the Gradients over all batches\n",
    "        if gradients is None:\n",
    "            gradients = batch_gradients\n",
    "        else:\n",
    "            gradients = addGradients(gradients, batch_gradients)\n",
    "\n",
    "    # For Polciy Gradients\n",
    "    averageGradients(gradients, N)\n",
    "    actor_optimizer.apply_gradients(zip(gradients, actor.trainable_variables))     \n",
    "\n",
    "    if step % 100 == 0:\n",
    "        print(\"Saving model, actor & critic @ timestep\", step)\n",
    "        actor.save_weights(STORE_PATH + f\"/actor{dt.datetime.now().strftime('%d%m%Y%H%M')}\")\n",
    "        # For Actor-Critic\n",
    "        # critic.save_weights(STORE_PATH + f\"/critic{dt.datetime.now().strftime('%d%m%Y%H%M')}\")\n",
    "\n",
    "    print(f\"Step: {step}, AvgReward: {avg_reward}, step: {step}\")\n",
    "    with logger.as_default():\n",
    "            tf.summary.scalar('avgReward', avg_reward, step=step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
