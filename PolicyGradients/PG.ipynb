{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cart Pole using Vanilla Policy Gradients\n",
    "\n",
    "We are using a **On-Policy** vanilla Policy Gradient which has the following implemented <br/>\n",
    "- **Causality** \n",
    "- Mean Rewards **Baselines**\n",
    "- **Discounting** since sum of rewards in an infinite horizon problem will explode the gradient\n",
    "- And Gradient **Averaging** across all mini-batches of the policy's trajectories to calculate expecations over all  gradients\n",
    "\n",
    "Following are NOT Implemented\n",
    "- **Parallelism** in sampling\n",
    "- Frequent model **Saving**\n",
    "- num_of_episodes **decaying** since episode length increases as the policy improves\n",
    "- Hyper-Parameter **Tuning** ... the following ideas in this [link](https://towardsdatascience.com/a-conceptual-explanation-of-bayesian-model-based-hyperparameter-optimization-for-machine-learning-b8172278050f) has to be used. \n",
    "\n",
    "The following tutorial on Gradient Averaging in the context of tensorflow's **keras** helped me complete this notebook\n",
    "https://medium.com/analytics-vidhya/tf-gradienttape-explained-for-keras-users-cc3f06276f22"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 2\n"
     ]
    }
   ],
   "source": [
    "GAMMA = 0.99\n",
    "\n",
    "env = gym.make(\"CartPole-v0\")\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n\n",
    "\n",
    "print(state_size, action_size)\n",
    "\n",
    "STORE_PATH = '/Users/SV/Desktop/Lyra/CS285/PolicyGradients'\n",
    "logger = tf.summary.create_file_writer(STORE_PATH + f\"/PG-CartPole_{dt.datetime.now().strftime('%d%m%Y%H%M')}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = keras.Sequential([\n",
    "    keras.layers.Dense(30, activation='relu', kernel_initializer=keras.initializers.he_normal()),\n",
    "    keras.layers.Dense(30, activation='relu', kernel_initializer=keras.initializers.he_normal()),\n",
    "    keras.layers.Dense(action_size, activation='softmax')\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Sample Action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_action(state):\n",
    "    # Since only one state is there, N = 1 in the mini-batch.\n",
    "    # And index 0, gives the action for that state.     \n",
    "    softmax_out = network(state.reshape((1, -1)))\n",
    "    selected_action = np.random.choice(action_size, p=softmax_out.numpy()[0])\n",
    "    return selected_action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Sample Episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rewards2go(rewards):\n",
    "\n",
    "    reward_sum = 0\n",
    "    result = []\n",
    "    for reward in reversed(rewards):\n",
    "        reward_sum = reward + GAMMA * reward_sum\n",
    "        result.append(reward_sum)\n",
    "    result.reverse()\n",
    "    return result\n",
    "\n",
    "def sample_episode(states, rewards, rewards2go, actions):\n",
    "    state = env.reset()\n",
    "    raw_rewards = []\n",
    "    while True:\n",
    "        action = sample_action(state)\n",
    "        next_state, reward, done, _ = env.step(action)        \n",
    "        \n",
    "        states.append(state)        \n",
    "        actions.append(action)\n",
    "        raw_rewards.append(reward)\n",
    "        state = next_state\n",
    "        \n",
    "        if done:\n",
    "            rewards.extend(raw_rewards)             \n",
    "            rewards2go.extend(get_rewards2go(raw_rewards))\n",
    "            return\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "Sampling Episode -  30\n",
      "Sampling Episode -  60\n",
      "Sampling Episode -  90\n",
      "Sampling Episode -  120\n",
      "Sampling Episode -  150\n",
      "Sampling Episode -  180\n",
      "Sampling Episode -  210\n",
      "Sampling Episode -  240\n",
      "Sampling Episode -  270\n",
      "Sampling Episode -  300\n",
      "Sampling Episode -  330\n",
      "Sampling Episode -  360\n",
      "Sampling Episode -  390\n",
      "Sampling Episode -  420\n",
      "Sampling Episode -  450\n",
      "Sampling Episode -  480\n",
      "Sampling Episode -  510\n",
      "Sampling Episode -  540\n",
      "Sampling Episode -  570\n",
      "Sampling Episode -  600\n",
      "Sampling Episode -  630\n",
      "Sampling Episode -  660\n",
      "Sampled 676 Episodes\n",
      "Saving model, actor & critic @ timestep 0\n",
      "Step: 0, AvgReward: 29.59467455621302, BatchLen: 3\n",
      "Sampling Episode -  30\n",
      "Sampling Episode -  60\n",
      "Sampling Episode -  90\n",
      "Sampling Episode -  120\n",
      "Sampling Episode -  150\n",
      "Sampling Episode -  180\n",
      "Sampling Episode -  210\n",
      "Sampling Episode -  240\n",
      "Sampling Episode -  270\n",
      "Sampling Episode -  300\n",
      "Sampling Episode -  330\n",
      "Sampling Episode -  360\n",
      "Sampling Episode -  390\n",
      "Sampling Episode -  420\n",
      "Sampling Episode -  450\n",
      "Sampling Episode -  480\n",
      "Sampling Episode -  510\n",
      "Sampling Episode -  540\n",
      "Sampling Episode -  570\n",
      "Sampling Episode -  600\n",
      "Sampled 618 Episodes\n",
      "Step: 1, AvgReward: 32.362459546925564, BatchLen: 3\n",
      "Sampling Episode -  30\n",
      "Sampling Episode -  60\n",
      "Sampling Episode -  90\n",
      "Sampling Episode -  120\n",
      "Sampling Episode -  150\n",
      "Sampling Episode -  180\n",
      "Sampling Episode -  210\n",
      "Sampling Episode -  240\n",
      "Sampling Episode -  270\n",
      "Sampling Episode -  300\n",
      "Sampling Episode -  330\n",
      "Sampling Episode -  360\n",
      "Sampling Episode -  390\n",
      "Sampling Episode -  420\n",
      "Sampling Episode -  450\n",
      "Sampling Episode -  480\n",
      "Sampling Episode -  510\n",
      "Sampling Episode -  540\n",
      "Sampled 566 Episodes\n",
      "Step: 2, AvgReward: 35.3886925795053, BatchLen: 3\n",
      "Sampling Episode -  30\n",
      "Sampling Episode -  60\n",
      "Sampling Episode -  90\n",
      "Sampling Episode -  120\n",
      "Sampling Episode -  150\n",
      "Sampling Episode -  180\n",
      "Sampling Episode -  210\n",
      "Sampling Episode -  240\n",
      "Sampling Episode -  270\n",
      "Sampling Episode -  300\n",
      "Sampling Episode -  330\n",
      "Sampling Episode -  360\n",
      "Sampling Episode -  390\n",
      "Sampling Episode -  420\n",
      "Sampling Episode -  450\n",
      "Sampling Episode -  480\n",
      "Sampling Episode -  510\n",
      "Sampled 535 Episodes\n",
      "Step: 3, AvgReward: 37.45607476635514, BatchLen: 3\n",
      "Sampling Episode -  30\n",
      "Sampling Episode -  60\n",
      "Sampling Episode -  90\n",
      "Sampling Episode -  120\n",
      "Sampling Episode -  150\n",
      "Sampling Episode -  180\n",
      "Sampling Episode -  210\n",
      "Sampling Episode -  240\n",
      "Sampling Episode -  270\n",
      "Sampling Episode -  300\n",
      "Sampling Episode -  330\n",
      "Sampling Episode -  360\n",
      "Sampling Episode -  390\n",
      "Sampling Episode -  420\n",
      "Sampling Episode -  450\n",
      "Sampling Episode -  480\n",
      "Sampling Episode -  510\n",
      "Sampled 518 Episodes\n",
      "Step: 4, AvgReward: 38.62741312741313, BatchLen: 3\n",
      "Sampling Episode -  30\n",
      "Sampling Episode -  60\n",
      "Sampling Episode -  90\n",
      "Sampling Episode -  120\n",
      "Sampling Episode -  150\n",
      "Sampling Episode -  180\n",
      "Sampling Episode -  210\n",
      "Sampling Episode -  240\n",
      "Sampling Episode -  270\n",
      "Sampling Episode -  300\n",
      "Sampling Episode -  330\n",
      "Sampling Episode -  360\n",
      "Sampling Episode -  390\n",
      "Sampling Episode -  420\n",
      "Sampling Episode -  450\n",
      "Sampling Episode -  480\n",
      "Sampled 485 Episodes\n",
      "Step: 5, AvgReward: 41.30309278350516, BatchLen: 3\n",
      "Sampling Episode -  30\n",
      "Sampling Episode -  60\n",
      "Sampling Episode -  90\n",
      "Sampling Episode -  120\n",
      "Sampling Episode -  150\n",
      "Sampling Episode -  180\n",
      "Sampling Episode -  210\n",
      "Sampling Episode -  240\n",
      "Sampling Episode -  270\n",
      "Sampling Episode -  300\n",
      "Sampling Episode -  330\n",
      "Sampling Episode -  360\n",
      "Sampling Episode -  390\n",
      "Sampling Episode -  420\n",
      "Sampled 441 Episodes\n",
      "Step: 6, AvgReward: 45.435374149659864, BatchLen: 3\n",
      "Sampling Episode -  30\n",
      "Sampling Episode -  60\n",
      "Sampling Episode -  90\n",
      "Sampling Episode -  120\n",
      "Sampling Episode -  150\n",
      "Sampling Episode -  180\n",
      "Sampling Episode -  210\n",
      "Sampling Episode -  240\n",
      "Sampling Episode -  270\n",
      "Sampling Episode -  300\n",
      "Sampling Episode -  330\n",
      "Sampling Episode -  360\n",
      "Sampling Episode -  390\n",
      "Sampling Episode -  420\n",
      "Sampled 423 Episodes\n",
      "Step: 7, AvgReward: 47.364066193853425, BatchLen: 3\n",
      "Sampling Episode -  30\n",
      "Sampling Episode -  60\n",
      "Sampling Episode -  90\n",
      "Sampling Episode -  120\n",
      "Sampling Episode -  150\n",
      "Sampling Episode -  180\n",
      "Sampling Episode -  210\n",
      "Sampling Episode -  240\n",
      "Sampling Episode -  270\n",
      "Sampling Episode -  300\n",
      "Sampling Episode -  330\n",
      "Sampling Episode -  360\n",
      "Sampled 363 Episodes\n",
      "Step: 8, AvgReward: 55.099173553719005, BatchLen: 3\n",
      "Sampling Episode -  30\n",
      "Sampling Episode -  60\n",
      "Sampling Episode -  90\n",
      "Sampling Episode -  120\n",
      "Sampling Episode -  150\n",
      "Sampling Episode -  180\n",
      "Sampling Episode -  210\n",
      "Sampling Episode -  240\n",
      "Sampling Episode -  270\n",
      "Sampling Episode -  300\n",
      "Sampling Episode -  330\n",
      "Sampling Episode -  360\n",
      "Sampled 368 Episodes\n",
      "Step: 9, AvgReward: 54.40217391304348, BatchLen: 3\n",
      "Sampling Episode -  30\n",
      "Sampling Episode -  60\n",
      "Sampling Episode -  90\n",
      "Sampling Episode -  120\n",
      "Sampling Episode -  150\n",
      "Sampling Episode -  180\n",
      "Sampling Episode -  210\n",
      "Sampling Episode -  240\n",
      "Sampling Episode -  270\n",
      "Sampling Episode -  300\n",
      "Sampling Episode -  330\n",
      "Sampled 342 Episodes\n",
      "Step: 10, AvgReward: 58.558479532163744, BatchLen: 3\n",
      "Sampling Episode -  30\n",
      "Sampling Episode -  60\n",
      "Sampling Episode -  90\n",
      "Sampling Episode -  120\n",
      "Sampling Episode -  150\n",
      "Sampling Episode -  180\n",
      "Sampling Episode -  210\n",
      "Sampling Episode -  240\n",
      "Sampling Episode -  270\n",
      "Sampling Episode -  300\n",
      "Sampled 313 Episodes\n",
      "Step: 11, AvgReward: 63.92332268370607, BatchLen: 3\n",
      "Sampling Episode -  30\n",
      "Sampling Episode -  60\n",
      "Sampling Episode -  90\n",
      "Sampling Episode -  120\n",
      "Sampling Episode -  150\n",
      "Sampling Episode -  180\n",
      "Sampling Episode -  210\n",
      "Sampling Episode -  240\n",
      "Sampling Episode -  270\n",
      "Sampled 295 Episodes\n",
      "Step: 12, AvgReward: 67.89152542372881, BatchLen: 3\n",
      "Sampling Episode -  30\n",
      "Sampling Episode -  60\n",
      "Sampling Episode -  90\n",
      "Sampling Episode -  120\n",
      "Sampling Episode -  150\n",
      "Sampling Episode -  180\n",
      "Sampling Episode -  210\n",
      "Sampling Episode -  240\n",
      "Sampling Episode -  270\n",
      "Sampled 295 Episodes\n",
      "Step: 13, AvgReward: 68.15593220338982, BatchLen: 3\n",
      "Sampling Episode -  30\n",
      "Sampling Episode -  60\n",
      "Sampling Episode -  90\n",
      "Sampling Episode -  120\n",
      "Sampling Episode -  150\n",
      "Sampling Episode -  180\n",
      "Sampling Episode -  210\n",
      "Sampling Episode -  240\n",
      "Sampled 267 Episodes\n",
      "Step: 14, AvgReward: 74.91760299625469, BatchLen: 3\n",
      "Sampling Episode -  30\n",
      "Sampling Episode -  60\n",
      "Sampling Episode -  90\n",
      "Sampling Episode -  120\n",
      "Sampling Episode -  150\n",
      "Sampling Episode -  180\n",
      "Sampling Episode -  210\n",
      "Sampling Episode -  240\n",
      "Sampling Episode -  270\n",
      "Sampled 273 Episodes\n",
      "Step: 15, AvgReward: 73.3076923076923, BatchLen: 3\n",
      "Sampling Episode -  30\n",
      "Sampling Episode -  60\n",
      "Sampling Episode -  90\n",
      "Sampling Episode -  120\n",
      "Sampling Episode -  150\n",
      "Sampling Episode -  180\n",
      "Sampling Episode -  210\n",
      "Sampling Episode -  240\n",
      "Sampled 258 Episodes\n",
      "Step: 16, AvgReward: 77.55813953488372, BatchLen: 3\n",
      "Sampling Episode -  30\n",
      "Sampling Episode -  60\n",
      "Sampling Episode -  90\n",
      "Sampling Episode -  120\n",
      "Sampling Episode -  150\n",
      "Sampling Episode -  180\n",
      "Sampling Episode -  210\n",
      "Sampling Episode -  240\n",
      "Sampled 253 Episodes\n",
      "Step: 17, AvgReward: 79.26877470355731, BatchLen: 3\n",
      "Sampling Episode -  30\n",
      "Sampling Episode -  60\n",
      "Sampling Episode -  90\n",
      "Sampling Episode -  120\n",
      "Sampling Episode -  150\n",
      "Sampling Episode -  180\n",
      "Sampling Episode -  210\n",
      "Sampled 233 Episodes\n",
      "Step: 18, AvgReward: 85.931330472103, BatchLen: 3\n",
      "Sampling Episode -  30\n",
      "Sampling Episode -  60\n",
      "Sampling Episode -  90\n",
      "Sampling Episode -  120\n",
      "Sampling Episode -  150\n",
      "Sampling Episode -  180\n",
      "Sampling Episode -  210\n",
      "Sampled 216 Episodes\n",
      "Step: 19, AvgReward: 92.74074074074075, BatchLen: 3\n",
      "Sampling Episode -  30\n",
      "Sampling Episode -  60\n",
      "Sampling Episode -  90\n",
      "Sampling Episode -  120\n",
      "Sampling Episode -  150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampling Episode -  180\n",
      "Sampling Episode -  210\n",
      "Sampled 211 Episodes\n",
      "Step: 20, AvgReward: 95.08056872037915, BatchLen: 3\n",
      "Sampling Episode -  30\n",
      "Sampling Episode -  60\n",
      "Sampling Episode -  90\n",
      "Sampling Episode -  120\n",
      "Sampling Episode -  150\n",
      "Sampling Episode -  180\n",
      "Sampling Episode -  210\n",
      "Sampled 214 Episodes\n",
      "Step: 21, AvgReward: 93.57943925233644, BatchLen: 3\n",
      "Sampling Episode -  30\n",
      "Sampling Episode -  60\n",
      "Sampling Episode -  90\n",
      "Sampling Episode -  120\n",
      "Sampling Episode -  150\n",
      "Sampling Episode -  180\n",
      "Sampled 197 Episodes\n",
      "Step: 22, AvgReward: 101.92893401015229, BatchLen: 3\n",
      "Sampling Episode -  30\n",
      "Sampling Episode -  60\n",
      "Sampling Episode -  90\n",
      "Sampling Episode -  120\n",
      "Sampling Episode -  150\n",
      "Sampling Episode -  180\n",
      "Sampled 186 Episodes\n",
      "Step: 23, AvgReward: 108.03763440860214, BatchLen: 3\n",
      "Sampling Episode -  30\n",
      "Sampling Episode -  60\n",
      "Sampling Episode -  90\n",
      "Sampling Episode -  120\n",
      "Sampling Episode -  150\n",
      "Sampling Episode -  180\n",
      "Sampled 183 Episodes\n",
      "Step: 24, AvgReward: 110.24043715846994, BatchLen: 3\n",
      "Sampling Episode -  30\n",
      "Sampling Episode -  60\n",
      "Sampling Episode -  90\n",
      "Sampling Episode -  120\n",
      "Sampling Episode -  150\n",
      "Sampled 162 Episodes\n",
      "Step: 25, AvgReward: 123.87654320987654, BatchLen: 3\n",
      "Sampling Episode -  30\n",
      "Sampling Episode -  60\n",
      "Sampling Episode -  90\n",
      "Sampling Episode -  120\n",
      "Sampling Episode -  150\n",
      "Sampled 163 Episodes\n",
      "Step: 26, AvgReward: 123.0920245398773, BatchLen: 3\n",
      "Sampling Episode -  30\n",
      "Sampling Episode -  60\n",
      "Sampling Episode -  90\n",
      "Sampling Episode -  120\n",
      "Sampling Episode -  150\n",
      "Sampled 173 Episodes\n",
      "Step: 27, AvgReward: 116.52023121387283, BatchLen: 3\n",
      "Sampling Episode -  30\n",
      "Sampling Episode -  60\n",
      "Sampling Episode -  90\n",
      "Sampling Episode -  120\n",
      "Sampling Episode -  150\n",
      "Sampled 157 Episodes\n",
      "Step: 28, AvgReward: 128.43949044585986, BatchLen: 3\n",
      "Sampling Episode -  30\n",
      "Sampling Episode -  60\n",
      "Sampling Episode -  90\n",
      "Sampling Episode -  120\n",
      "Sampled 146 Episodes\n",
      "Step: 29, AvgReward: 137.37671232876713, BatchLen: 3\n",
      "Sampling Episode -  30\n",
      "Sampling Episode -  60\n",
      "Sampling Episode -  90\n",
      "Sampling Episode -  120\n",
      "Sampling Episode -  150\n",
      "Sampled 153 Episodes\n",
      "Step: 30, AvgReward: 130.96732026143792, BatchLen: 3\n",
      "Sampling Episode -  30\n",
      "Sampling Episode -  60\n",
      "Sampling Episode -  90\n",
      "Sampling Episode -  120\n",
      "Sampled 143 Episodes\n",
      "Step: 31, AvgReward: 140.36363636363637, BatchLen: 3\n",
      "Sampling Episode -  30\n",
      "Sampling Episode -  60\n",
      "Sampling Episode -  90\n",
      "Sampling Episode -  120\n",
      "Sampled 139 Episodes\n",
      "Step: 32, AvgReward: 144.53237410071944, BatchLen: 3\n",
      "Sampling Episode -  30\n",
      "Sampling Episode -  60\n",
      "Sampling Episode -  90\n",
      "Sampling Episode -  120\n",
      "Sampled 138 Episodes\n",
      "Step: 33, AvgReward: 145.02173913043478, BatchLen: 3\n",
      "Sampling Episode -  30\n",
      "Sampling Episode -  60\n",
      "Sampling Episode -  90\n",
      "Sampling Episode -  120\n",
      "Sampled 133 Episodes\n",
      "Step: 34, AvgReward: 151.64661654135338, BatchLen: 3\n",
      "Sampling Episode -  30\n",
      "Sampling Episode -  60\n",
      "Sampling Episode -  90\n",
      "Sampling Episode -  120\n",
      "Sampled 126 Episodes\n",
      "Step: 35, AvgReward: 160.015873015873, BatchLen: 3\n",
      "Sampling Episode -  30\n",
      "Sampling Episode -  60\n",
      "Sampling Episode -  90\n",
      "Sampling Episode -  120\n",
      "Sampled 129 Episodes\n",
      "Step: 36, AvgReward: 156.07751937984497, BatchLen: 3\n",
      "Sampling Episode -  30\n",
      "Sampling Episode -  60\n",
      "Sampling Episode -  90\n",
      "Sampling Episode -  120\n",
      "Sampled 130 Episodes\n",
      "Step: 37, AvgReward: 154.04615384615386, BatchLen: 3\n",
      "Sampling Episode -  30\n",
      "Sampling Episode -  60\n",
      "Sampling Episode -  90\n",
      "Sampling Episode -  120\n",
      "Sampled 123 Episodes\n",
      "Step: 38, AvgReward: 163.47967479674796, BatchLen: 3\n",
      "Sampling Episode -  30\n",
      "Sampling Episode -  60\n",
      "Sampling Episode -  90\n",
      "Sampling Episode -  120\n",
      "Sampled 125 Episodes\n",
      "Step: 39, AvgReward: 160.728, BatchLen: 3\n",
      "Sampling Episode -  30\n",
      "Sampling Episode -  60\n",
      "Sampling Episode -  90\n",
      "Sampling Episode -  120\n",
      "Sampled 122 Episodes\n",
      "Step: 40, AvgReward: 164.01639344262296, BatchLen: 3\n",
      "Sampling Episode -  30\n",
      "Sampling Episode -  60\n",
      "Sampling Episode -  90\n",
      "Sampled 116 Episodes\n",
      "Step: 41, AvgReward: 172.75862068965517, BatchLen: 3\n",
      "Sampling Episode -  30\n",
      "Sampling Episode -  60\n",
      "Sampling Episode -  90\n",
      "Sampled 112 Episodes\n",
      "Step: 42, AvgReward: 178.8125, BatchLen: 3\n",
      "Sampling Episode -  30\n",
      "Sampling Episode -  60\n",
      "Sampling Episode -  90\n",
      "Sampled 116 Episodes\n",
      "Step: 43, AvgReward: 173.06896551724137, BatchLen: 3\n",
      "Sampling Episode -  30\n",
      "Sampling Episode -  60\n",
      "Sampling Episode -  90\n",
      "Sampled 116 Episodes\n",
      "Step: 44, AvgReward: 172.70689655172413, BatchLen: 3\n",
      "Sampling Episode -  30\n",
      "Sampling Episode -  60\n",
      "Sampling Episode -  90\n",
      "Sampled 113 Episodes\n",
      "Step: 45, AvgReward: 177.94690265486724, BatchLen: 3\n",
      "Sampling Episode -  30\n",
      "Sampling Episode -  60\n",
      "Sampling Episode -  90\n",
      "Sampled 111 Episodes\n",
      "Step: 46, AvgReward: 180.4144144144144, BatchLen: 3\n",
      "Sampling Episode -  30\n",
      "Sampling Episode -  60\n",
      "Sampling Episode -  90\n",
      "Sampled 110 Episodes\n",
      "Step: 47, AvgReward: 182.15454545454546, BatchLen: 3\n",
      "Sampling Episode -  30\n",
      "Sampling Episode -  60\n",
      "Sampling Episode -  90\n",
      "Sampled 114 Episodes\n",
      "Step: 48, AvgReward: 176.640350877193, BatchLen: 3\n",
      "Sampling Episode -  30\n",
      "Sampling Episode -  60\n",
      "Sampling Episode -  90\n",
      "Sampled 108 Episodes\n",
      "Step: 49, AvgReward: 186.14814814814815, BatchLen: 3\n",
      "Sampling Episode -  30\n",
      "Sampling Episode -  60\n",
      "Sampling Episode -  90\n",
      "Sampled 106 Episodes\n",
      "Step: 50, AvgReward: 190.03773584905662, BatchLen: 3\n",
      "Sampling Episode -  30\n",
      "Sampling Episode -  60\n",
      "Sampling Episode -  90\n",
      "Sampled 109 Episodes\n",
      "Step: 51, AvgReward: 184.1743119266055, BatchLen: 3\n",
      "Sampling Episode -  30\n",
      "Sampling Episode -  60\n",
      "Sampling Episode -  90\n",
      "Sampled 108 Episodes\n",
      "Step: 52, AvgReward: 186.4351851851852, BatchLen: 3\n",
      "Sampling Episode -  30\n",
      "Sampling Episode -  60\n",
      "Sampling Episode -  90\n",
      "Sampled 109 Episodes\n",
      "Step: 53, AvgReward: 185.13761467889907, BatchLen: 3\n",
      "Sampling Episode -  30\n",
      "Sampling Episode -  60\n",
      "Sampling Episode -  90\n",
      "Sampled 107 Episodes\n",
      "Step: 54, AvgReward: 187.69158878504672, BatchLen: 3\n",
      "Sampling Episode -  30\n",
      "Sampling Episode -  60\n",
      "Sampling Episode -  90\n",
      "Sampled 107 Episodes\n",
      "Step: 55, AvgReward: 187.58878504672896, BatchLen: 3\n",
      "Sampling Episode -  30\n",
      "Sampling Episode -  60\n",
      "Sampling Episode -  90\n",
      "Sampled 104 Episodes\n",
      "Step: 56, AvgReward: 192.90384615384616, BatchLen: 3\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-a7c4ffaf44ef>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0mN\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;32mwhile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0msample_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0msample_episode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards2go\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m         \u001b[0mN\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;36m30\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-b94f08174b49>\u001b[0m in \u001b[0;36msample_episode\u001b[0;34m(states, rewards, rewards2go, actions)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mraw_rewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-c8351aa0a54d>\u001b[0m in \u001b[0;36msample_action\u001b[0;34m(state)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;31m# Since only one state is there, N = 1 in the mini-batch.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;31m# And index 0, gives the action for that state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0msoftmax_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mselected_action\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msoftmax_out\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mselected_action\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    820\u001b[0m           with base_layer_utils.autocast_context_manager(\n\u001b[1;32m    821\u001b[0m               self._compute_dtype):\n\u001b[0;32m--> 822\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    823\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_activity_regularization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    824\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_mask_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/sequential.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, training, mask)\u001b[0m\n\u001b[1;32m    279\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'training'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    280\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 281\u001b[0;31m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    282\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m       \u001b[0;31m# `outputs` will be the inputs to the next layer.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    822\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    823\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_activity_regularization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 824\u001b[0;31m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_mask_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    825\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m_set_mask_metadata\u001b[0;34m(self, inputs, outputs, previous_mask)\u001b[0m\n\u001b[1;32m   1950\u001b[0m         \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1951\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1952\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mare_all_symbolic_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1953\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mflat_outputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1954\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_keras_mask'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/utils/tf_utils.py\u001b[0m in \u001b[0;36mare_all_symbolic_tensors\u001b[0;34m(tensors)\u001b[0m\n\u001b[1;32m    314\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mare_all_symbolic_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 316\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_symbolic_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtensor\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    317\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "sample_size = 20000\n",
    "batch_size = 5120\n",
    "steps = 1000\n",
    "\n",
    "network_optimizer = tf.keras.optimizers.Adam(lr=0.001)\n",
    "\n",
    "def averageGradients(grads, N):\n",
    "    for i in range(len(grads)):\n",
    "        grads[i] = grads[i]/N\n",
    "        \n",
    "        \n",
    "def addGradients(grads, batch_grads):\n",
    "    sum_grad = []\n",
    "    for (grad, batch_grad) in zip(grads, batch_grads):\n",
    "        sum_grad.append(grad + batch_grad)\n",
    "    return sum_grad\n",
    "\n",
    "def generate_sample_batcher(states, rewards2go, actions):\n",
    "    rewards2go = np.array(rewards2go)\n",
    "    states = np.array(states)\n",
    "    actions = np.array(actions)\n",
    "    \n",
    "    baseline = np.mean(rewards2go)\n",
    "    rewards2go = rewards2go - baseline\n",
    "        \n",
    "    def sample_batcher(n):\n",
    "        return states[n:n+batch_size], rewards2go[n:n+batch_size], actions[n:n+batch_size]\n",
    "\n",
    "    return sample_batcher\n",
    "\n",
    "for step in range(steps):\n",
    "    \n",
    "    # This is one gradient step    \n",
    "    rewards = []\n",
    "    rewards2go = []\n",
    "    states = []\n",
    "    actions = []\n",
    "    sum_reward = 0\n",
    "\n",
    "    N = 0 \n",
    "    while(len(states) < sample_size):\n",
    "        sample_episode(states, rewards, rewards2go, actions)\n",
    "        N += 1\n",
    "        if(N%30 == 0):\n",
    "            print(\"Sampling Episode - \", N)\n",
    "    print(\"Sampled\", N, \"Episodes\")\n",
    "    \n",
    "    \n",
    "    avg_reward = np.sum(np.array(rewards)) / N\n",
    "    bat_per_epoch = math.floor(len(states) / batch_size)\n",
    "    sample_batcher = generate_sample_batcher(states, rewards2go, actions)\n",
    "    \n",
    "    gradients = None\n",
    "    for i in range(bat_per_epoch):\n",
    "        n = i*batch_size\n",
    "        batch_states, batch_rewards2go, batch_actions = sample_batcher(n)\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions = network(batch_states)\n",
    "            loss = tf.keras.losses.sparse_categorical_crossentropy(y_true=batch_actions, y_pred=predictions, from_logits=False)\n",
    "            weighted_loss = loss * batch_rewards2go\n",
    "        batch_gradients = tape.gradient(weighted_loss, network.trainable_variables)\n",
    "\n",
    "        if gradients is None:\n",
    "            gradients = batch_gradients\n",
    "        else:\n",
    "            gradients = addGradients(gradients, batch_gradients)\n",
    "    \n",
    "    averageGradients(gradients, N)\n",
    "    network_optimizer.apply_gradients(zip(gradients, network.trainable_variables)) \n",
    "\n",
    "    if step % 100 == 0:\n",
    "        print(\"Saving model, actor & critic @ timestep\", step)\n",
    "        network.save_weights(STORE_PATH + f\"/network{dt.datetime.now().strftime('%d%m%Y%H%M')}\")\n",
    "        # For Actor-Critic\n",
    "        # critic.save_weights(STORE_PATH + f\"/critic{dt.datetime.now().strftime('%d%m%Y%H%M')}\")\n",
    "\n",
    "    print(f\"Step: {step}, AvgReward: {avg_reward}, BatchLen: {bat_per_epoch}\")\n",
    "    with logger.as_default():\n",
    "            tf.summary.scalar('avgReward', avg_reward, step=step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
