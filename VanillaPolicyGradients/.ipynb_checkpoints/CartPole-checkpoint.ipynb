{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cart Pole using Vanilla Policy Gradients\n",
    "\n",
    "We are using a **On-Policy** vanilla Policy Gradient which has the following implemented <br/>\n",
    "- **Causality** \n",
    "- Mean Rewards **Baselines**\n",
    "- **Discount** since sum of rewards in an infinite horizon problem will explode the gradient\n",
    "- And Gradient **Averaging** across all mini-batches of the policy's trajectories.\n",
    "\n",
    "Following are NOT Implemented\n",
    "- **Parallelism** in sampling\n",
    "- Frequent model **Saving**\n",
    "- num_of_episodes **decaying** (Reason - As policy improves, episode length increases making training slow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 2\n"
     ]
    }
   ],
   "source": [
    "GAMMA = 0.95\n",
    "\n",
    "env = gym.make(\"CartPole-v0\")\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n\n",
    "\n",
    "print(state_size, action_size)\n",
    "\n",
    "STORE_PATH = '/Users/SV/Desktop/Lyra/CS285/PolicyGradients'\n",
    "logger = tf.summary.create_file_writer(STORE_PATH + f\"/PGCartPole_{dt.datetime.now().strftime('%d%m%Y%H%M')}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = keras.Sequential([\n",
    "    keras.layers.Dense(30, activation='relu', kernel_initializer=keras.initializers.he_normal()),\n",
    "    keras.layers.Dense(30, activation='relu', kernel_initializer=keras.initializers.he_normal()),\n",
    "    keras.layers.Dense(action_size, activation='softmax')\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Sample Action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_action(state):\n",
    "    # Since only one state is there, N = 1 in the mini-batch.\n",
    "    # And index 0, gives the action for that state.     \n",
    "    softmax_out = network(state.reshape((1, -1)))\n",
    "    selected_action = np.random.choice(action_size, p=softmax_out.numpy()[0])\n",
    "    return selected_action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Sample Episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discounted_rewardsToGo(rewards):\n",
    "\n",
    "    reward_sum = 0\n",
    "    result = []\n",
    "    for reward in reversed(rewards):\n",
    "        reward_sum = reward + GAMMA * reward_sum\n",
    "        result.append(reward_sum)\n",
    "    result = np.array(result)\n",
    "    # result -= np.mean(result)\n",
    "    # result /= np.std(result)\n",
    "    return result[::-1] \n",
    "\n",
    "def sample_episode(states, rewards, actions):\n",
    "    state = env.reset()\n",
    "    raw_rewards = []\n",
    "    while True:\n",
    "        states.append(state)\n",
    "        action = sample_action(state)\n",
    "        actions.append(action)\n",
    "        state, reward, done, _ = env.step(action)\n",
    "        raw_rewards.append(reward)\n",
    "        \n",
    "        if done:\n",
    "            # loss = one_episode_gradient_update(network, rewards, states, actions, num_actions)\n",
    "            tot_reward = sum(raw_rewards)\n",
    "            episode_rewards = discounted_rewardsToGo(raw_rewards)\n",
    "            rewards.extend(episode_rewards) \n",
    "            return tot_reward\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "Sampling Episode -  0\n",
      "Sampling Episode -  30\n",
      "Sampling Episode -  60\n",
      "Sampling Episode -  90\n",
      "Sampling Episode -  120\n",
      "Sampling Episode -  150\n",
      "Sampling Episode -  180\n",
      "Sampling Episode -  210\n",
      "Sampling Episode -  240\n",
      "Sampling Episode -  270\n",
      "Step: 0, AvgReward: 24.30666666666667, BatchLen: 28\n",
      "Sampling Episode -  0\n",
      "Sampling Episode -  30\n",
      "Sampling Episode -  60\n",
      "Sampling Episode -  90\n",
      "Sampling Episode -  120\n",
      "Sampling Episode -  150\n",
      "Sampling Episode -  180\n",
      "Sampling Episode -  210\n",
      "Sampling Episode -  240\n",
      "Sampling Episode -  270\n",
      "Step: 1, AvgReward: 25.956666666666667, BatchLen: 30\n",
      "Sampling Episode -  0\n",
      "Sampling Episode -  30\n",
      "Sampling Episode -  60\n",
      "Sampling Episode -  90\n",
      "Sampling Episode -  120\n",
      "Sampling Episode -  150\n",
      "Sampling Episode -  180\n",
      "Sampling Episode -  210\n",
      "Sampling Episode -  240\n",
      "Sampling Episode -  270\n",
      "Step: 2, AvgReward: 26.623333333333335, BatchLen: 31\n",
      "Sampling Episode -  0\n",
      "Sampling Episode -  30\n",
      "Sampling Episode -  60\n",
      "Sampling Episode -  90\n",
      "Sampling Episode -  120\n",
      "Sampling Episode -  150\n",
      "Sampling Episode -  180\n",
      "Sampling Episode -  210\n",
      "Sampling Episode -  240\n",
      "Sampling Episode -  270\n",
      "Step: 3, AvgReward: 27.156666666666666, BatchLen: 31\n",
      "Sampling Episode -  0\n",
      "Sampling Episode -  30\n",
      "Sampling Episode -  60\n",
      "Sampling Episode -  90\n",
      "Sampling Episode -  120\n",
      "Sampling Episode -  150\n",
      "Sampling Episode -  180\n",
      "Sampling Episode -  210\n",
      "Sampling Episode -  240\n",
      "Sampling Episode -  270\n",
      "Step: 4, AvgReward: 29.323333333333334, BatchLen: 34\n",
      "Sampling Episode -  0\n",
      "Sampling Episode -  30\n",
      "Sampling Episode -  60\n",
      "Sampling Episode -  90\n",
      "Sampling Episode -  120\n",
      "Sampling Episode -  150\n",
      "Sampling Episode -  180\n",
      "Sampling Episode -  210\n",
      "Sampling Episode -  240\n",
      "Sampling Episode -  270\n",
      "Step: 5, AvgReward: 30.173333333333332, BatchLen: 35\n",
      "Sampling Episode -  0\n",
      "Sampling Episode -  30\n",
      "Sampling Episode -  60\n",
      "Sampling Episode -  90\n",
      "Sampling Episode -  120\n",
      "Sampling Episode -  150\n",
      "Sampling Episode -  180\n",
      "Sampling Episode -  210\n",
      "Sampling Episode -  240\n",
      "Sampling Episode -  270\n",
      "Step: 6, AvgReward: 32.26, BatchLen: 37\n",
      "Sampling Episode -  0\n",
      "Sampling Episode -  30\n",
      "Sampling Episode -  60\n",
      "Sampling Episode -  90\n",
      "Sampling Episode -  120\n",
      "Sampling Episode -  150\n",
      "Sampling Episode -  180\n",
      "Sampling Episode -  210\n",
      "Sampling Episode -  240\n",
      "Sampling Episode -  270\n",
      "Step: 7, AvgReward: 31.94, BatchLen: 37\n",
      "Sampling Episode -  0\n",
      "Sampling Episode -  30\n",
      "Sampling Episode -  60\n",
      "Sampling Episode -  90\n",
      "Sampling Episode -  120\n",
      "Sampling Episode -  150\n",
      "Sampling Episode -  180\n",
      "Sampling Episode -  210\n",
      "Sampling Episode -  240\n",
      "Sampling Episode -  270\n",
      "Step: 8, AvgReward: 32.96666666666667, BatchLen: 38\n",
      "Sampling Episode -  0\n",
      "Sampling Episode -  30\n",
      "Sampling Episode -  60\n",
      "Sampling Episode -  90\n",
      "Sampling Episode -  120\n",
      "Sampling Episode -  150\n",
      "Sampling Episode -  180\n",
      "Sampling Episode -  210\n",
      "Sampling Episode -  240\n",
      "Sampling Episode -  270\n",
      "Step: 9, AvgReward: 35.04666666666667, BatchLen: 41\n",
      "Sampling Episode -  0\n",
      "Sampling Episode -  30\n",
      "Sampling Episode -  60\n",
      "Sampling Episode -  90\n",
      "Sampling Episode -  120\n",
      "Sampling Episode -  150\n",
      "Sampling Episode -  180\n",
      "Sampling Episode -  210\n",
      "Sampling Episode -  240\n",
      "Sampling Episode -  270\n",
      "Step: 10, AvgReward: 38.29333333333334, BatchLen: 44\n",
      "Sampling Episode -  0\n",
      "Sampling Episode -  30\n",
      "Sampling Episode -  60\n",
      "Sampling Episode -  90\n",
      "Sampling Episode -  120\n",
      "Sampling Episode -  150\n",
      "Sampling Episode -  180\n",
      "Sampling Episode -  210\n",
      "Sampling Episode -  240\n",
      "Sampling Episode -  270\n",
      "Step: 11, AvgReward: 36.99, BatchLen: 43\n",
      "Sampling Episode -  0\n",
      "Sampling Episode -  30\n",
      "Sampling Episode -  60\n",
      "Sampling Episode -  90\n",
      "Sampling Episode -  120\n",
      "Sampling Episode -  150\n",
      "Sampling Episode -  180\n",
      "Sampling Episode -  210\n",
      "Sampling Episode -  240\n",
      "Sampling Episode -  270\n",
      "Step: 12, AvgReward: 41.02, BatchLen: 48\n",
      "Sampling Episode -  0\n",
      "Sampling Episode -  30\n",
      "Sampling Episode -  60\n",
      "Sampling Episode -  90\n",
      "Sampling Episode -  120\n",
      "Sampling Episode -  150\n",
      "Sampling Episode -  180\n",
      "Sampling Episode -  210\n",
      "Sampling Episode -  240\n",
      "Sampling Episode -  270\n",
      "Step: 13, AvgReward: 42.75, BatchLen: 50\n",
      "Sampling Episode -  0\n",
      "Sampling Episode -  30\n",
      "Sampling Episode -  60\n",
      "Sampling Episode -  90\n",
      "Sampling Episode -  120\n",
      "Sampling Episode -  150\n",
      "Sampling Episode -  180\n",
      "Sampling Episode -  210\n",
      "Sampling Episode -  240\n",
      "Sampling Episode -  270\n",
      "Step: 14, AvgReward: 45.47, BatchLen: 53\n",
      "Sampling Episode -  0\n",
      "Sampling Episode -  30\n",
      "Sampling Episode -  60\n",
      "Sampling Episode -  90\n",
      "Sampling Episode -  120\n",
      "Sampling Episode -  150\n",
      "Sampling Episode -  180\n",
      "Sampling Episode -  210\n",
      "Sampling Episode -  240\n",
      "Sampling Episode -  270\n",
      "Step: 15, AvgReward: 46.12, BatchLen: 54\n",
      "Sampling Episode -  0\n",
      "Sampling Episode -  30\n",
      "Sampling Episode -  60\n",
      "Sampling Episode -  90\n",
      "Sampling Episode -  120\n",
      "Sampling Episode -  150\n",
      "Sampling Episode -  180\n",
      "Sampling Episode -  210\n",
      "Sampling Episode -  240\n",
      "Sampling Episode -  270\n",
      "Step: 16, AvgReward: 44.74666666666667, BatchLen: 52\n",
      "Sampling Episode -  0\n",
      "Sampling Episode -  30\n",
      "Sampling Episode -  60\n",
      "Sampling Episode -  90\n",
      "Sampling Episode -  120\n",
      "Sampling Episode -  150\n",
      "Sampling Episode -  180\n",
      "Sampling Episode -  210\n",
      "Sampling Episode -  240\n",
      "Sampling Episode -  270\n",
      "Step: 17, AvgReward: 50.60666666666667, BatchLen: 59\n",
      "Sampling Episode -  0\n",
      "Sampling Episode -  30\n",
      "Sampling Episode -  60\n",
      "Sampling Episode -  90\n",
      "Sampling Episode -  120\n",
      "Sampling Episode -  150\n",
      "Sampling Episode -  180\n",
      "Sampling Episode -  210\n",
      "Sampling Episode -  240\n",
      "Sampling Episode -  270\n",
      "Step: 18, AvgReward: 50.52, BatchLen: 59\n",
      "Sampling Episode -  0\n",
      "Sampling Episode -  30\n",
      "Sampling Episode -  60\n",
      "Sampling Episode -  90\n",
      "Sampling Episode -  120\n",
      "Sampling Episode -  150\n",
      "Sampling Episode -  180\n",
      "Sampling Episode -  210\n",
      "Sampling Episode -  240\n",
      "Sampling Episode -  270\n",
      "Step: 19, AvgReward: 49.61666666666667, BatchLen: 58\n",
      "Sampling Episode -  0\n",
      "Sampling Episode -  30\n",
      "Sampling Episode -  60\n",
      "Sampling Episode -  90\n",
      "Sampling Episode -  120\n",
      "Sampling Episode -  150\n",
      "Sampling Episode -  180\n",
      "Sampling Episode -  210\n",
      "Sampling Episode -  240\n",
      "Sampling Episode -  270\n",
      "Step: 20, AvgReward: 55.45, BatchLen: 64\n",
      "Sampling Episode -  0\n",
      "Sampling Episode -  30\n",
      "Sampling Episode -  60\n",
      "Sampling Episode -  90\n",
      "Sampling Episode -  120\n",
      "Sampling Episode -  150\n",
      "Sampling Episode -  180\n",
      "Sampling Episode -  210\n",
      "Sampling Episode -  240\n",
      "Sampling Episode -  270\n",
      "Step: 21, AvgReward: 54.85, BatchLen: 64\n",
      "Sampling Episode -  0\n",
      "Sampling Episode -  30\n",
      "Sampling Episode -  60\n",
      "Sampling Episode -  90\n",
      "Sampling Episode -  120\n",
      "Sampling Episode -  150\n",
      "Sampling Episode -  180\n",
      "Sampling Episode -  210\n",
      "Sampling Episode -  240\n",
      "Sampling Episode -  270\n",
      "Step: 22, AvgReward: 56.196666666666665, BatchLen: 65\n",
      "Sampling Episode -  0\n",
      "Sampling Episode -  30\n",
      "Sampling Episode -  60\n",
      "Sampling Episode -  90\n",
      "Sampling Episode -  120\n",
      "Sampling Episode -  150\n",
      "Sampling Episode -  180\n",
      "Sampling Episode -  210\n",
      "Sampling Episode -  240\n",
      "Sampling Episode -  270\n",
      "Step: 23, AvgReward: 57.53666666666667, BatchLen: 67\n",
      "Sampling Episode -  0\n",
      "Sampling Episode -  30\n",
      "Sampling Episode -  60\n",
      "Sampling Episode -  90\n",
      "Sampling Episode -  120\n",
      "Sampling Episode -  150\n",
      "Sampling Episode -  180\n",
      "Sampling Episode -  210\n",
      "Sampling Episode -  240\n",
      "Sampling Episode -  270\n",
      "Step: 24, AvgReward: 64.67, BatchLen: 75\n",
      "Sampling Episode -  0\n",
      "Sampling Episode -  30\n",
      "Sampling Episode -  60\n",
      "Sampling Episode -  90\n",
      "Sampling Episode -  120\n",
      "Sampling Episode -  150\n",
      "Sampling Episode -  180\n",
      "Sampling Episode -  210\n",
      "Sampling Episode -  240\n",
      "Sampling Episode -  270\n",
      "Step: 25, AvgReward: 61.74666666666667, BatchLen: 72\n",
      "Sampling Episode -  0\n",
      "Sampling Episode -  30\n",
      "Sampling Episode -  60\n",
      "Sampling Episode -  90\n",
      "Sampling Episode -  120\n",
      "Sampling Episode -  150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampling Episode -  180\n",
      "Sampling Episode -  210\n",
      "Sampling Episode -  240\n",
      "Sampling Episode -  270\n",
      "Step: 26, AvgReward: 69.99666666666667, BatchLen: 82\n",
      "Sampling Episode -  0\n",
      "Sampling Episode -  30\n",
      "Sampling Episode -  60\n",
      "Sampling Episode -  90\n",
      "Sampling Episode -  120\n",
      "Sampling Episode -  150\n",
      "Sampling Episode -  180\n",
      "Sampling Episode -  210\n",
      "Sampling Episode -  240\n",
      "Sampling Episode -  270\n",
      "Step: 27, AvgReward: 66.07666666666667, BatchLen: 77\n",
      "Sampling Episode -  0\n",
      "Sampling Episode -  30\n",
      "Sampling Episode -  60\n",
      "Sampling Episode -  90\n",
      "Sampling Episode -  120\n",
      "Sampling Episode -  150\n",
      "Sampling Episode -  180\n",
      "Sampling Episode -  210\n",
      "Sampling Episode -  240\n",
      "Sampling Episode -  270\n",
      "Step: 28, AvgReward: 68.47666666666667, BatchLen: 80\n",
      "Sampling Episode -  0\n",
      "Sampling Episode -  30\n",
      "Sampling Episode -  60\n",
      "Sampling Episode -  90\n",
      "Sampling Episode -  120\n",
      "Sampling Episode -  150\n",
      "Sampling Episode -  180\n",
      "Sampling Episode -  210\n",
      "Sampling Episode -  240\n",
      "Sampling Episode -  270\n",
      "Step: 29, AvgReward: 77.17333333333333, BatchLen: 90\n",
      "Sampling Episode -  0\n",
      "Sampling Episode -  30\n",
      "Sampling Episode -  60\n",
      "Sampling Episode -  90\n",
      "Sampling Episode -  120\n",
      "Sampling Episode -  150\n",
      "Sampling Episode -  180\n",
      "Sampling Episode -  210\n",
      "Sampling Episode -  240\n",
      "Sampling Episode -  270\n",
      "Step: 30, AvgReward: 79.68, BatchLen: 93\n",
      "Sampling Episode -  0\n",
      "Sampling Episode -  30\n",
      "Sampling Episode -  60\n",
      "Sampling Episode -  90\n",
      "Sampling Episode -  120\n",
      "Sampling Episode -  150\n",
      "Sampling Episode -  180\n",
      "Sampling Episode -  210\n",
      "Sampling Episode -  240\n",
      "Sampling Episode -  270\n",
      "Step: 31, AvgReward: 84.95666666666666, BatchLen: 99\n",
      "Sampling Episode -  0\n",
      "Sampling Episode -  30\n",
      "Sampling Episode -  60\n",
      "Sampling Episode -  90\n",
      "Sampling Episode -  120\n",
      "Sampling Episode -  150\n",
      "Sampling Episode -  180\n",
      "Sampling Episode -  210\n",
      "Sampling Episode -  240\n",
      "Sampling Episode -  270\n",
      "Step: 32, AvgReward: 82.65, BatchLen: 96\n",
      "Sampling Episode -  0\n",
      "Sampling Episode -  30\n",
      "Sampling Episode -  60\n",
      "Sampling Episode -  90\n",
      "Sampling Episode -  120\n",
      "Sampling Episode -  150\n",
      "Sampling Episode -  180\n",
      "Sampling Episode -  210\n",
      "Sampling Episode -  240\n",
      "Sampling Episode -  270\n",
      "Step: 33, AvgReward: 90.11333333333333, BatchLen: 105\n",
      "Sampling Episode -  0\n",
      "Sampling Episode -  30\n",
      "Sampling Episode -  60\n",
      "Sampling Episode -  90\n",
      "Sampling Episode -  120\n",
      "Sampling Episode -  150\n",
      "Sampling Episode -  180\n",
      "Sampling Episode -  210\n",
      "Sampling Episode -  240\n",
      "Sampling Episode -  270\n",
      "Step: 34, AvgReward: 91.84666666666666, BatchLen: 107\n",
      "Sampling Episode -  0\n",
      "Sampling Episode -  30\n",
      "Sampling Episode -  60\n",
      "Sampling Episode -  90\n",
      "Sampling Episode -  120\n",
      "Sampling Episode -  150\n",
      "Sampling Episode -  180\n",
      "Sampling Episode -  210\n",
      "Sampling Episode -  240\n",
      "Sampling Episode -  270\n",
      "Step: 35, AvgReward: 94.17666666666666, BatchLen: 110\n",
      "Sampling Episode -  0\n",
      "Sampling Episode -  30\n",
      "Sampling Episode -  60\n",
      "Sampling Episode -  90\n",
      "Sampling Episode -  120\n",
      "Sampling Episode -  150\n",
      "Sampling Episode -  180\n",
      "Sampling Episode -  210\n",
      "Sampling Episode -  240\n",
      "Sampling Episode -  270\n",
      "Step: 36, AvgReward: 100.90666666666667, BatchLen: 118\n",
      "Sampling Episode -  0\n",
      "Sampling Episode -  30\n",
      "Sampling Episode -  60\n",
      "Sampling Episode -  90\n",
      "Sampling Episode -  120\n",
      "Sampling Episode -  150\n",
      "Sampling Episode -  180\n",
      "Sampling Episode -  210\n",
      "Sampling Episode -  240\n",
      "Sampling Episode -  270\n",
      "Step: 37, AvgReward: 106.99666666666667, BatchLen: 125\n",
      "Sampling Episode -  0\n",
      "Sampling Episode -  30\n",
      "Sampling Episode -  60\n",
      "Sampling Episode -  90\n",
      "Sampling Episode -  120\n",
      "Sampling Episode -  150\n",
      "Sampling Episode -  180\n",
      "Sampling Episode -  210\n",
      "Sampling Episode -  240\n",
      "Sampling Episode -  270\n",
      "Step: 38, AvgReward: 108.66666666666667, BatchLen: 127\n",
      "Sampling Episode -  0\n",
      "Sampling Episode -  30\n",
      "Sampling Episode -  60\n",
      "Sampling Episode -  90\n",
      "Sampling Episode -  120\n",
      "Sampling Episode -  150\n",
      "Sampling Episode -  180\n",
      "Sampling Episode -  210\n",
      "Sampling Episode -  240\n",
      "Sampling Episode -  270\n",
      "Step: 39, AvgReward: 114.87, BatchLen: 134\n",
      "Sampling Episode -  0\n",
      "Sampling Episode -  30\n",
      "Sampling Episode -  60\n",
      "Sampling Episode -  90\n",
      "Sampling Episode -  120\n",
      "Sampling Episode -  150\n",
      "Sampling Episode -  180\n",
      "Sampling Episode -  210\n",
      "Sampling Episode -  240\n",
      "Sampling Episode -  270\n",
      "Step: 40, AvgReward: 121.44333333333333, BatchLen: 142\n",
      "Sampling Episode -  0\n",
      "Sampling Episode -  30\n",
      "Sampling Episode -  60\n",
      "Sampling Episode -  90\n",
      "Sampling Episode -  120\n",
      "Sampling Episode -  150\n",
      "Sampling Episode -  180\n",
      "Sampling Episode -  210\n",
      "Sampling Episode -  240\n",
      "Sampling Episode -  270\n",
      "Step: 41, AvgReward: 120.27666666666667, BatchLen: 140\n",
      "Sampling Episode -  0\n",
      "Sampling Episode -  30\n",
      "Sampling Episode -  60\n",
      "Sampling Episode -  90\n",
      "Sampling Episode -  120\n",
      "Sampling Episode -  150\n",
      "Sampling Episode -  180\n",
      "Sampling Episode -  210\n",
      "Sampling Episode -  240\n",
      "Sampling Episode -  270\n",
      "Step: 42, AvgReward: 130.56666666666666, BatchLen: 153\n",
      "Sampling Episode -  0\n",
      "Sampling Episode -  30\n",
      "Sampling Episode -  60\n",
      "Sampling Episode -  90\n",
      "Sampling Episode -  120\n",
      "Sampling Episode -  150\n",
      "Sampling Episode -  180\n",
      "Sampling Episode -  210\n",
      "Sampling Episode -  240\n",
      "Sampling Episode -  270\n",
      "Step: 43, AvgReward: 134.57666666666665, BatchLen: 157\n",
      "Sampling Episode -  0\n",
      "Sampling Episode -  30\n",
      "Sampling Episode -  60\n",
      "Sampling Episode -  90\n",
      "Sampling Episode -  120\n",
      "Sampling Episode -  150\n",
      "Sampling Episode -  180\n",
      "Sampling Episode -  210\n",
      "Sampling Episode -  240\n",
      "Sampling Episode -  270\n",
      "Step: 44, AvgReward: 135.66666666666666, BatchLen: 158\n",
      "Sampling Episode -  0\n",
      "Sampling Episode -  30\n",
      "Sampling Episode -  60\n",
      "Sampling Episode -  90\n",
      "Sampling Episode -  120\n",
      "Sampling Episode -  150\n",
      "Sampling Episode -  180\n",
      "Sampling Episode -  210\n",
      "Sampling Episode -  240\n",
      "Sampling Episode -  270\n",
      "Step: 45, AvgReward: 145.39, BatchLen: 170\n",
      "Sampling Episode -  0\n",
      "Sampling Episode -  30\n",
      "Sampling Episode -  60\n",
      "Sampling Episode -  90\n",
      "Sampling Episode -  120\n",
      "Sampling Episode -  150\n",
      "Sampling Episode -  180\n",
      "Sampling Episode -  210\n",
      "Sampling Episode -  240\n",
      "Sampling Episode -  270\n",
      "Step: 46, AvgReward: 148.84666666666666, BatchLen: 174\n",
      "Sampling Episode -  0\n",
      "Sampling Episode -  30\n",
      "Sampling Episode -  60\n",
      "Sampling Episode -  90\n",
      "Sampling Episode -  120\n",
      "Sampling Episode -  150\n",
      "Sampling Episode -  180\n",
      "Sampling Episode -  210\n",
      "Sampling Episode -  240\n",
      "Sampling Episode -  270\n",
      "Step: 47, AvgReward: 149.59, BatchLen: 175\n",
      "Sampling Episode -  0\n",
      "Sampling Episode -  30\n",
      "Sampling Episode -  60\n",
      "Sampling Episode -  90\n",
      "Sampling Episode -  120\n",
      "Sampling Episode -  150\n",
      "Sampling Episode -  180\n",
      "Sampling Episode -  210\n",
      "Sampling Episode -  240\n",
      "Sampling Episode -  270\n",
      "Step: 48, AvgReward: 154.51333333333332, BatchLen: 181\n",
      "Sampling Episode -  0\n",
      "Sampling Episode -  30\n",
      "Sampling Episode -  60\n",
      "Sampling Episode -  90\n",
      "Sampling Episode -  120\n",
      "Sampling Episode -  150\n",
      "Sampling Episode -  180\n",
      "Sampling Episode -  210\n",
      "Sampling Episode -  240\n",
      "Sampling Episode -  270\n",
      "Step: 49, AvgReward: 156.83333333333334, BatchLen: 183\n",
      "Sampling Episode -  0\n",
      "Sampling Episode -  30\n",
      "Sampling Episode -  60\n",
      "Sampling Episode -  90\n",
      "Sampling Episode -  120\n",
      "Sampling Episode -  150\n",
      "Sampling Episode -  180\n",
      "Sampling Episode -  210\n",
      "Sampling Episode -  240\n",
      "Sampling Episode -  270\n",
      "Step: 50, AvgReward: 165.39, BatchLen: 193\n",
      "Sampling Episode -  0\n",
      "Sampling Episode -  30\n",
      "Sampling Episode -  60\n",
      "Sampling Episode -  90\n",
      "Sampling Episode -  120\n",
      "Sampling Episode -  150\n",
      "Sampling Episode -  180\n",
      "Sampling Episode -  210\n",
      "Sampling Episode -  240\n",
      "Sampling Episode -  270\n",
      "Step: 51, AvgReward: 165.57333333333332, BatchLen: 194\n",
      "Sampling Episode -  0\n",
      "Sampling Episode -  30\n",
      "Sampling Episode -  60\n",
      "Sampling Episode -  90\n",
      "Sampling Episode -  120\n",
      "Sampling Episode -  150\n",
      "Sampling Episode -  180\n",
      "Sampling Episode -  210\n",
      "Sampling Episode -  240\n",
      "Sampling Episode -  270\n",
      "Step: 52, AvgReward: 165.85, BatchLen: 194\n",
      "Sampling Episode -  0\n",
      "Sampling Episode -  30\n",
      "Sampling Episode -  60\n",
      "Sampling Episode -  90\n",
      "Sampling Episode -  120\n",
      "Sampling Episode -  150\n",
      "Sampling Episode -  180\n",
      "Sampling Episode -  210\n",
      "Sampling Episode -  240\n",
      "Sampling Episode -  270\n",
      "Step: 53, AvgReward: 171.4, BatchLen: 200\n",
      "Sampling Episode -  0\n",
      "Sampling Episode -  30\n",
      "Sampling Episode -  60\n",
      "Sampling Episode -  90\n",
      "Sampling Episode -  120\n",
      "Sampling Episode -  150\n",
      "Sampling Episode -  180\n",
      "Sampling Episode -  210\n",
      "Sampling Episode -  240\n",
      "Sampling Episode -  270\n",
      "Step: 54, AvgReward: 173.99333333333334, BatchLen: 203\n",
      "Sampling Episode -  0\n",
      "Sampling Episode -  30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampling Episode -  60\n",
      "Sampling Episode -  90\n",
      "Sampling Episode -  120\n",
      "Sampling Episode -  150\n",
      "Sampling Episode -  180\n",
      "Sampling Episode -  210\n",
      "Sampling Episode -  240\n",
      "Sampling Episode -  270\n",
      "Step: 55, AvgReward: 176.93666666666667, BatchLen: 207\n",
      "Sampling Episode -  0\n",
      "Sampling Episode -  30\n",
      "Sampling Episode -  60\n",
      "Sampling Episode -  90\n",
      "Sampling Episode -  120\n",
      "Sampling Episode -  150\n",
      "Sampling Episode -  180\n",
      "Sampling Episode -  210\n",
      "Sampling Episode -  240\n",
      "Sampling Episode -  270\n",
      "Step: 56, AvgReward: 179.86333333333334, BatchLen: 210\n",
      "Sampling Episode -  0\n",
      "Sampling Episode -  30\n",
      "Sampling Episode -  60\n",
      "Sampling Episode -  90\n",
      "Sampling Episode -  120\n",
      "Sampling Episode -  150\n",
      "Sampling Episode -  180\n",
      "Sampling Episode -  210\n",
      "Sampling Episode -  240\n",
      "Sampling Episode -  270\n",
      "Step: 57, AvgReward: 181.14666666666668, BatchLen: 212\n",
      "Sampling Episode -  0\n",
      "Sampling Episode -  30\n",
      "Sampling Episode -  60\n",
      "Sampling Episode -  90\n",
      "Sampling Episode -  120\n",
      "Sampling Episode -  150\n",
      "Sampling Episode -  180\n",
      "Sampling Episode -  210\n",
      "Sampling Episode -  240\n",
      "Sampling Episode -  270\n",
      "Step: 58, AvgReward: 180.78666666666666, BatchLen: 211\n",
      "Sampling Episode -  0\n",
      "Sampling Episode -  30\n",
      "Sampling Episode -  60\n",
      "Sampling Episode -  90\n",
      "Sampling Episode -  120\n",
      "Sampling Episode -  150\n",
      "Sampling Episode -  180\n",
      "Sampling Episode -  210\n",
      "Sampling Episode -  240\n",
      "Sampling Episode -  270\n",
      "Step: 59, AvgReward: 184.03333333333333, BatchLen: 215\n",
      "Sampling Episode -  0\n",
      "Sampling Episode -  30\n",
      "Sampling Episode -  60\n",
      "Sampling Episode -  90\n",
      "Sampling Episode -  120\n",
      "Sampling Episode -  150\n",
      "Sampling Episode -  180\n",
      "Sampling Episode -  210\n",
      "Sampling Episode -  240\n",
      "Sampling Episode -  270\n",
      "Step: 60, AvgReward: 186.53, BatchLen: 218\n",
      "Sampling Episode -  0\n",
      "Sampling Episode -  30\n",
      "Sampling Episode -  60\n",
      "Sampling Episode -  90\n",
      "Sampling Episode -  120\n",
      "Sampling Episode -  150\n",
      "Sampling Episode -  180\n",
      "Sampling Episode -  210\n",
      "Sampling Episode -  240\n",
      "Sampling Episode -  270\n",
      "Step: 61, AvgReward: 185.92333333333335, BatchLen: 217\n",
      "Sampling Episode -  0\n",
      "Sampling Episode -  30\n",
      "Sampling Episode -  60\n",
      "Sampling Episode -  90\n",
      "Sampling Episode -  120\n",
      "Sampling Episode -  150\n",
      "Sampling Episode -  180\n",
      "Sampling Episode -  210\n",
      "Sampling Episode -  240\n",
      "Sampling Episode -  270\n",
      "Step: 62, AvgReward: 187.07333333333332, BatchLen: 219\n",
      "Sampling Episode -  0\n",
      "Sampling Episode -  30\n",
      "Sampling Episode -  60\n",
      "Sampling Episode -  90\n",
      "Sampling Episode -  120\n",
      "Sampling Episode -  150\n",
      "Sampling Episode -  180\n",
      "Sampling Episode -  210\n",
      "Sampling Episode -  240\n",
      "Sampling Episode -  270\n",
      "Step: 63, AvgReward: 187.8, BatchLen: 220\n",
      "Sampling Episode -  0\n",
      "Sampling Episode -  30\n",
      "Sampling Episode -  60\n",
      "Sampling Episode -  90\n",
      "Sampling Episode -  120\n",
      "Sampling Episode -  150\n",
      "Sampling Episode -  180\n",
      "Sampling Episode -  210\n",
      "Sampling Episode -  240\n",
      "Sampling Episode -  270\n",
      "Step: 64, AvgReward: 189.35333333333332, BatchLen: 221\n",
      "Sampling Episode -  0\n",
      "Sampling Episode -  30\n",
      "Sampling Episode -  60\n",
      "Sampling Episode -  90\n",
      "Sampling Episode -  120\n",
      "Sampling Episode -  150\n",
      "Sampling Episode -  180\n",
      "Sampling Episode -  210\n",
      "Sampling Episode -  240\n",
      "Sampling Episode -  270\n",
      "Step: 65, AvgReward: 190.11666666666667, BatchLen: 222\n",
      "Sampling Episode -  0\n",
      "Sampling Episode -  30\n",
      "Sampling Episode -  60\n",
      "Sampling Episode -  90\n",
      "Sampling Episode -  120\n",
      "Sampling Episode -  150\n",
      "Sampling Episode -  180\n",
      "Sampling Episode -  210\n",
      "Sampling Episode -  240\n",
      "Sampling Episode -  270\n",
      "Step: 66, AvgReward: 191.54, BatchLen: 224\n",
      "Sampling Episode -  0\n",
      "Sampling Episode -  30\n",
      "Sampling Episode -  60\n",
      "Sampling Episode -  90\n",
      "Sampling Episode -  120\n",
      "Sampling Episode -  150\n",
      "Sampling Episode -  180\n",
      "Sampling Episode -  210\n",
      "Sampling Episode -  240\n",
      "Sampling Episode -  270\n",
      "Step: 67, AvgReward: 192.50333333333333, BatchLen: 225\n",
      "Sampling Episode -  0\n",
      "Sampling Episode -  30\n",
      "Sampling Episode -  60\n",
      "Sampling Episode -  90\n",
      "Sampling Episode -  120\n",
      "Sampling Episode -  150\n",
      "Sampling Episode -  180\n",
      "Sampling Episode -  210\n",
      "Sampling Episode -  240\n",
      "Sampling Episode -  270\n",
      "Step: 68, AvgReward: 193.45, BatchLen: 226\n",
      "Sampling Episode -  0\n",
      "Sampling Episode -  30\n",
      "Sampling Episode -  60\n",
      "Sampling Episode -  90\n",
      "Sampling Episode -  120\n",
      "Sampling Episode -  150\n",
      "Sampling Episode -  180\n",
      "Sampling Episode -  210\n",
      "Sampling Episode -  240\n",
      "Sampling Episode -  270\n",
      "Step: 69, AvgReward: 190.57, BatchLen: 223\n",
      "Sampling Episode -  0\n",
      "Sampling Episode -  30\n",
      "Sampling Episode -  60\n",
      "Sampling Episode -  90\n",
      "Sampling Episode -  120\n",
      "Sampling Episode -  150\n",
      "Sampling Episode -  180\n",
      "Sampling Episode -  210\n",
      "Sampling Episode -  240\n",
      "Sampling Episode -  270\n",
      "Step: 70, AvgReward: 193.72666666666666, BatchLen: 227\n",
      "Sampling Episode -  0\n",
      "Sampling Episode -  30\n",
      "Sampling Episode -  60\n",
      "Sampling Episode -  90\n",
      "Sampling Episode -  120\n",
      "Sampling Episode -  150\n",
      "Sampling Episode -  180\n",
      "Sampling Episode -  210\n",
      "Sampling Episode -  240\n",
      "Sampling Episode -  270\n",
      "Step: 71, AvgReward: 193.71666666666667, BatchLen: 227\n",
      "Sampling Episode -  0\n",
      "Sampling Episode -  30\n",
      "Sampling Episode -  60\n",
      "Sampling Episode -  90\n",
      "Sampling Episode -  120\n",
      "Sampling Episode -  150\n",
      "Sampling Episode -  180\n",
      "Sampling Episode -  210\n",
      "Sampling Episode -  240\n",
      "Sampling Episode -  270\n",
      "Step: 72, AvgReward: 192.14333333333335, BatchLen: 225\n",
      "Sampling Episode -  0\n",
      "Sampling Episode -  30\n",
      "Sampling Episode -  60\n",
      "Sampling Episode -  90\n",
      "Sampling Episode -  120\n",
      "Sampling Episode -  150\n",
      "Sampling Episode -  180\n",
      "Sampling Episode -  210\n",
      "Sampling Episode -  240\n",
      "Sampling Episode -  270\n",
      "Step: 73, AvgReward: 193.59333333333333, BatchLen: 226\n",
      "Sampling Episode -  0\n",
      "Sampling Episode -  30\n",
      "Sampling Episode -  60\n",
      "Sampling Episode -  90\n",
      "Sampling Episode -  120\n",
      "Sampling Episode -  150\n",
      "Sampling Episode -  180\n",
      "Sampling Episode -  210\n",
      "Sampling Episode -  240\n",
      "Sampling Episode -  270\n",
      "Step: 74, AvgReward: 194.86333333333334, BatchLen: 228\n",
      "Sampling Episode -  0\n",
      "Sampling Episode -  30\n",
      "Sampling Episode -  60\n",
      "Sampling Episode -  90\n",
      "Sampling Episode -  120\n",
      "Sampling Episode -  150\n",
      "Sampling Episode -  180\n",
      "Sampling Episode -  210\n",
      "Sampling Episode -  240\n",
      "Sampling Episode -  270\n",
      "Step: 75, AvgReward: 195.43333333333334, BatchLen: 229\n",
      "Sampling Episode -  0\n",
      "Sampling Episode -  30\n",
      "Sampling Episode -  60\n",
      "Sampling Episode -  90\n",
      "Sampling Episode -  120\n",
      "Sampling Episode -  150\n",
      "Sampling Episode -  180\n",
      "Sampling Episode -  210\n",
      "Sampling Episode -  240\n",
      "Sampling Episode -  270\n",
      "Step: 76, AvgReward: 194.33333333333334, BatchLen: 227\n",
      "Sampling Episode -  0\n",
      "Sampling Episode -  30\n",
      "Sampling Episode -  60\n",
      "Sampling Episode -  90\n",
      "Sampling Episode -  120\n",
      "Sampling Episode -  150\n",
      "Sampling Episode -  180\n",
      "Sampling Episode -  210\n",
      "Sampling Episode -  240\n",
      "Sampling Episode -  270\n",
      "Step: 77, AvgReward: 195.61, BatchLen: 229\n",
      "Sampling Episode -  0\n",
      "Sampling Episode -  30\n",
      "Sampling Episode -  60\n",
      "Sampling Episode -  90\n",
      "Sampling Episode -  120\n",
      "Sampling Episode -  150\n",
      "Sampling Episode -  180\n",
      "Sampling Episode -  210\n",
      "Sampling Episode -  240\n",
      "Sampling Episode -  270\n",
      "Step: 78, AvgReward: 195.23333333333332, BatchLen: 228\n",
      "Sampling Episode -  0\n",
      "Sampling Episode -  30\n",
      "Sampling Episode -  60\n",
      "Sampling Episode -  90\n",
      "Sampling Episode -  120\n",
      "Sampling Episode -  150\n",
      "Sampling Episode -  180\n",
      "Sampling Episode -  210\n",
      "Sampling Episode -  240\n",
      "Sampling Episode -  270\n",
      "Step: 79, AvgReward: 196.91333333333333, BatchLen: 230\n",
      "Sampling Episode -  0\n",
      "Sampling Episode -  30\n",
      "Sampling Episode -  60\n",
      "Sampling Episode -  90\n",
      "Sampling Episode -  120\n",
      "Sampling Episode -  150\n",
      "Sampling Episode -  180\n",
      "Sampling Episode -  210\n",
      "Sampling Episode -  240\n",
      "Sampling Episode -  270\n",
      "Step: 80, AvgReward: 194.59333333333333, BatchLen: 228\n",
      "Sampling Episode -  0\n",
      "Sampling Episode -  30\n",
      "Sampling Episode -  60\n",
      "Sampling Episode -  90\n",
      "Sampling Episode -  120\n",
      "Sampling Episode -  150\n",
      "Sampling Episode -  180\n",
      "Sampling Episode -  210\n",
      "Sampling Episode -  240\n",
      "Sampling Episode -  270\n",
      "Step: 81, AvgReward: 194.57333333333332, BatchLen: 228\n",
      "Sampling Episode -  0\n",
      "Sampling Episode -  30\n",
      "Sampling Episode -  60\n",
      "Sampling Episode -  90\n",
      "Sampling Episode -  120\n",
      "Sampling Episode -  150\n",
      "Sampling Episode -  180\n",
      "Sampling Episode -  210\n",
      "Sampling Episode -  240\n",
      "Sampling Episode -  270\n",
      "Step: 82, AvgReward: 195.24666666666667, BatchLen: 228\n",
      "Sampling Episode -  0\n",
      "Sampling Episode -  30\n",
      "Sampling Episode -  60\n",
      "Sampling Episode -  90\n",
      "Sampling Episode -  120\n",
      "Sampling Episode -  150\n",
      "Sampling Episode -  180\n",
      "Sampling Episode -  210\n",
      "Sampling Episode -  240\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampling Episode -  270\n",
      "Step: 83, AvgReward: 194.9, BatchLen: 228\n",
      "Sampling Episode -  0\n",
      "Sampling Episode -  30\n",
      "Sampling Episode -  60\n",
      "Sampling Episode -  90\n",
      "Sampling Episode -  120\n",
      "Sampling Episode -  150\n",
      "Sampling Episode -  180\n",
      "Sampling Episode -  210\n",
      "Sampling Episode -  240\n",
      "Sampling Episode -  270\n",
      "Step: 84, AvgReward: 197.30666666666667, BatchLen: 231\n",
      "Sampling Episode -  0\n",
      "Sampling Episode -  30\n",
      "Sampling Episode -  60\n",
      "Sampling Episode -  90\n",
      "Sampling Episode -  120\n",
      "Sampling Episode -  150\n",
      "Sampling Episode -  180\n",
      "Sampling Episode -  210\n",
      "Sampling Episode -  240\n",
      "Sampling Episode -  270\n",
      "Step: 85, AvgReward: 195.81666666666666, BatchLen: 229\n",
      "Sampling Episode -  0\n",
      "Sampling Episode -  30\n",
      "Sampling Episode -  60\n",
      "Sampling Episode -  90\n",
      "Sampling Episode -  120\n",
      "Sampling Episode -  150\n",
      "Sampling Episode -  180\n",
      "Sampling Episode -  210\n",
      "Sampling Episode -  240\n",
      "Sampling Episode -  270\n",
      "Step: 86, AvgReward: 194.98666666666668, BatchLen: 228\n",
      "Sampling Episode -  0\n",
      "Sampling Episode -  30\n",
      "Sampling Episode -  60\n",
      "Sampling Episode -  90\n",
      "Sampling Episode -  120\n",
      "Sampling Episode -  150\n",
      "Sampling Episode -  180\n",
      "Sampling Episode -  210\n",
      "Sampling Episode -  240\n",
      "Sampling Episode -  270\n",
      "Step: 87, AvgReward: 195.84, BatchLen: 229\n",
      "Sampling Episode -  0\n",
      "Sampling Episode -  30\n",
      "Sampling Episode -  60\n",
      "Sampling Episode -  90\n",
      "Sampling Episode -  120\n",
      "Sampling Episode -  150\n",
      "Sampling Episode -  180\n",
      "Sampling Episode -  210\n",
      "Sampling Episode -  240\n",
      "Sampling Episode -  270\n",
      "Step: 88, AvgReward: 196.84333333333333, BatchLen: 230\n",
      "Sampling Episode -  0\n",
      "Sampling Episode -  30\n",
      "Sampling Episode -  60\n",
      "Sampling Episode -  90\n",
      "Sampling Episode -  120\n",
      "Sampling Episode -  150\n",
      "Sampling Episode -  180\n",
      "Sampling Episode -  210\n",
      "Sampling Episode -  240\n",
      "Sampling Episode -  270\n",
      "Step: 89, AvgReward: 196.82333333333332, BatchLen: 230\n",
      "Sampling Episode -  0\n",
      "Sampling Episode -  30\n",
      "Sampling Episode -  60\n",
      "Sampling Episode -  90\n",
      "Sampling Episode -  120\n",
      "Sampling Episode -  150\n",
      "Sampling Episode -  180\n",
      "Sampling Episode -  210\n",
      "Sampling Episode -  240\n",
      "Sampling Episode -  270\n",
      "Step: 90, AvgReward: 197.80333333333334, BatchLen: 231\n",
      "Sampling Episode -  0\n",
      "Sampling Episode -  30\n",
      "Sampling Episode -  60\n",
      "Sampling Episode -  90\n",
      "Sampling Episode -  120\n",
      "Sampling Episode -  150\n",
      "Sampling Episode -  180\n",
      "Sampling Episode -  210\n",
      "Sampling Episode -  240\n",
      "Sampling Episode -  270\n",
      "Step: 91, AvgReward: 197.78, BatchLen: 231\n",
      "Sampling Episode -  0\n",
      "Sampling Episode -  30\n",
      "Sampling Episode -  60\n",
      "Sampling Episode -  90\n",
      "Sampling Episode -  120\n",
      "Sampling Episode -  150\n",
      "Sampling Episode -  180\n",
      "Sampling Episode -  210\n",
      "Sampling Episode -  240\n",
      "Sampling Episode -  270\n",
      "Step: 92, AvgReward: 198.09, BatchLen: 232\n",
      "Sampling Episode -  0\n",
      "Sampling Episode -  30\n",
      "Sampling Episode -  60\n",
      "Sampling Episode -  90\n",
      "Sampling Episode -  120\n",
      "Sampling Episode -  150\n",
      "Sampling Episode -  180\n",
      "Sampling Episode -  210\n",
      "Sampling Episode -  240\n",
      "Sampling Episode -  270\n",
      "Step: 93, AvgReward: 194.51666666666668, BatchLen: 227\n",
      "Sampling Episode -  0\n",
      "Sampling Episode -  30\n",
      "Sampling Episode -  60\n",
      "Sampling Episode -  90\n",
      "Sampling Episode -  120\n",
      "Sampling Episode -  150\n",
      "Sampling Episode -  180\n",
      "Sampling Episode -  210\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-e36ce86b9772>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepisode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_episodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0msum_reward\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0msample_episode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m         \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepisode\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;36m30\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Sampling Episode - \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepisode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-c221844a6fa4>\u001b[0m in \u001b[0;36msample_episode\u001b[0;34m(states, rewards, actions)\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mstates\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0mactions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-c8351aa0a54d>\u001b[0m in \u001b[0;36msample_action\u001b[0;34m(state)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;31m# Since only one state is there, N = 1 in the mini-batch.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;31m# And index 0, gives the action for that state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0msoftmax_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mselected_action\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msoftmax_out\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mselected_action\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    820\u001b[0m           with base_layer_utils.autocast_context_manager(\n\u001b[1;32m    821\u001b[0m               self._compute_dtype):\n\u001b[0;32m--> 822\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    823\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_activity_regularization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    824\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_mask_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/sequential.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, training, mask)\u001b[0m\n\u001b[1;32m    279\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'training'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    280\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 281\u001b[0;31m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    282\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m       \u001b[0;31m# `outputs` will be the inputs to the next layer.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    820\u001b[0m           with base_layer_utils.autocast_context_manager(\n\u001b[1;32m    821\u001b[0m               self._compute_dtype):\n\u001b[0;32m--> 822\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    823\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_activity_regularization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    824\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_mask_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/layers/core.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   1140\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msparse_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse_tensor_dense_matmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1141\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1142\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmat_mul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1143\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_bias\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1144\u001b[0m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias_add\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/ops/gen_math_ops.py\u001b[0m in \u001b[0;36mmat_mul\u001b[0;34m(a, b, transpose_a, transpose_b, name)\u001b[0m\n\u001b[1;32m   5604\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_context_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"MatMul\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5605\u001b[0m         \u001b[0mtld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mop_callbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"transpose_a\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtranspose_a\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"transpose_b\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5606\u001b[0;31m         transpose_b)\n\u001b[0m\u001b[1;32m   5607\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5608\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_episodes = 300\n",
    "steps = 500\n",
    "batch_size = 256\n",
    "optimizer = tf.keras.optimizers.Adam(lr=0.001)\n",
    "\n",
    "def averageGradients(grads):\n",
    "    for i in range(len(grads)):\n",
    "        grads[i] = grads[i]/num_episodes\n",
    "        \n",
    "        \n",
    "def addGradients(grads, batch_grads):\n",
    "    sum_grad = []\n",
    "    for (grad, batch_grad) in zip(grads, batch_grads):\n",
    "        sum_grad.append(grad + batch_grad)\n",
    "    return sum_grad\n",
    "\n",
    "def batch_step(states, actions, rewards):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = network(states)\n",
    "        loss = tf.keras.losses.sparse_categorical_crossentropy(y_true=actions, y_pred=predictions, from_logits=False)\n",
    "        weighted_loss = loss * rewards\n",
    "    batch_gradients = tape.gradient(weighted_loss, network.trainable_variables)\n",
    "    return batch_gradients\n",
    "\n",
    "for step in range(steps):\n",
    "    \n",
    "    # This is one gradient step    \n",
    "    rewards = []\n",
    "    states = []\n",
    "    actions = []\n",
    "    sum_reward = 0\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        sum_reward += sample_episode(states,rewards,actions)\n",
    "        if(episode%30 == 0):\n",
    "            print(\"Sampling Episode - \", episode)\n",
    "    \n",
    "    baseline = np.mean(np.array(rewards))\n",
    "    for i in range(len(rewards)):\n",
    "        rewards[i] -= baseline\n",
    "    \n",
    "    gradients = None\n",
    "    bat_per_epoch = math.floor(len(states) / batch_size)\n",
    "    for i in range(bat_per_epoch):\n",
    "        n = i*batch_size\n",
    "        states_np = np.array(states[n:n+batch_size])\n",
    "        actions_np = np.array(actions[n:n+batch_size])\n",
    "        rewards_np = np.array(rewards[n:n+batch_size])\n",
    "        batch_gradients = batch_step(states_np, actions_np, rewards_np)\n",
    "\n",
    "        if gradients is None:\n",
    "            gradients = batch_gradients\n",
    "        else:\n",
    "            gradients = addGradients(gradients, batch_gradients)\n",
    "    \n",
    "    averageGradients(gradients)\n",
    "    optimizer.apply_gradients(zip(gradients, network.trainable_variables)) \n",
    "    \n",
    "    avg_reward = sum_reward / num_episodes\n",
    "    print(f\"Step: {step}, AvgReward: {avg_reward}, BatchLen: {bat_per_epoch}\")\n",
    "    with logger.as_default():\n",
    "            tf.summary.scalar('avgReward', avg_reward, step=step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
